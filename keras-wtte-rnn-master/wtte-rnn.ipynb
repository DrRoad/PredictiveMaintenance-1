{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Masking\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as k\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Discrete log-likelihood for Weibull hazard function on censored survival data\n",
    "    y_true is a (samples, 2) tensor containing time-to-event (y), and an event indicator (u)\n",
    "    ab_pred is a (samples, 2) tensor containing predicted Weibull alpha (a) and beta (b) parameters\n",
    "    For math, see https://ragulpr.github.io/assets/draft_master_thesis_martinsson_egil_wtte_rnn_2016.pdf (Page 35)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weibull_loglik_discrete(y_true, ab_pred, name=None):\n",
    "    y_ = y_true[:, 0]\n",
    "    u_ = y_true[:, 1]\n",
    "    a_ = ab_pred[:, 0]\n",
    "    b_ = ab_pred[:, 1]\n",
    "\n",
    "    hazard0 = k.pow((y_ + 1e-35) / a_, b_)\n",
    "    hazard1 = k.pow((y_ + 1) / a_, b_)\n",
    "\n",
    "    return -1 * k.mean(u_ * k.log(k.exp(hazard1 - hazard0) - 1.0) - hazard1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Keras activation function, outputs alpha neuron using exponentiation and beta using softplus\n",
    "def activate(ab):\n",
    "    a = k.exp(ab[:, 0])\n",
    "    b = k.softplus(ab[:, 1])\n",
    "\n",
    "    a = k.reshape(a, (k.shape(a)[0], 1))\n",
    "    b = k.reshape(b, (k.shape(b)[0], 1))\n",
    "\n",
    "    return k.concatenate((a, b), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Load and parse engine data files into:\n",
    "       - an (engine/day, observed history, sensor readings) x tensor, where observed history is 100 days, zero-padded\n",
    "         for days that don't have a full 100 days of observed history (e.g., first observed day for an engine)\n",
    "       - an (engine/day, 2) tensor containing time-to-event and 1 (since all engines failed)\n",
    "\n",
    "    There are probably MUCH better ways of doing this, but I don't use Numpy that much, and the data parsing isn't the\n",
    "    point of this demo anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(name):\n",
    "    with open(name, 'r') as file:\n",
    "        return np.loadtxt(file, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, threshold=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_file('train.csv')\n",
    "test_x = load_file('test_x.csv')\n",
    "test_y = load_file('test_y.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the X values to normalize them, then split them back out\n",
    "all_x = np.concatenate((train[:, 2:26], test_x[:, 2:26]))\n",
    "#all_x = normalize(all_x, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[:, 2:26] = all_x[0:train.shape[0], :]\n",
    "test_x[:, 2:26] = all_x[train.shape[0]:, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurable observation look-back period for each engine/day\n",
    "max_time = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(engine, time, x, max_time, is_test):\n",
    "    # y[0] will be days remaining, y[1] will be event indicator, always 1 for this data\n",
    "    out_y = np.empty((0, 2), dtype=np.float32)\n",
    "\n",
    "    # A full history of sensor readings to date for each x\n",
    "    out_x = np.empty((0, max_time, 24), dtype=np.float32)\n",
    "\n",
    "    for i in range(100):\n",
    "        print(\"Engine: \" + str(i))\n",
    "        # When did the engine fail? (Last day + 1 for train data, irrelevant for test.)\n",
    "        max_engine_time = int(np.max(time[engine == i])) + 1\n",
    "\n",
    "        if is_test:\n",
    "            start = max_engine_time - 1\n",
    "        else:\n",
    "            start = 0\n",
    "\n",
    "        this_x = np.empty((0, max_time, 24), dtype=np.float32)\n",
    "\n",
    "        for j in range(start, max_engine_time):\n",
    "            engine_x = x[engine == i]\n",
    "\n",
    "            out_y = np.append(out_y, np.array((max_engine_time - j, 1), ndmin=2), axis=0)\n",
    "\n",
    "            xtemp = np.zeros((1, max_time, 24))\n",
    "            xtemp[:, max_time-min(j, 99)-1:max_time, :] = engine_x[max(0, j-max_time+1):j+1, :]\n",
    "            this_x = np.concatenate((this_x, xtemp))\n",
    "\n",
    "        out_x = np.concatenate((out_x, this_x))\n",
    "\n",
    "    return out_x, out_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y = build_data(train[:, 0], train[:, 1], train[:, 2:26], max_time, False)\n",
    "test_x = build_data(test_x[:, 0], test_x[:, 1], test_x[:, 2:26], max_time, True)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_u = np.zeros((100, 1), dtype=np.float32)\n",
    "train_u += 1\n",
    "test_y = np.append(np.reshape(test_y, (100, 1)), train_u, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start building our model\n",
    "model = Sequential()\n",
    "\n",
    "# Mask parts of the lookback period that are all zeros (i.e., unobserved) so they don't skew the model\n",
    "model.add(Masking(mask_value=0., input_shape=(max_time, 24)))\n",
    "\n",
    "# LSTM is just a common type of RNN. You could also try anything else (e.g., GRU).\n",
    "model.add(LSTM(20, input_dim= 24))\n",
    "\n",
    "# We need 2 neurons to output Alpha and Beta parameters for our Weibull distribution\n",
    "model.add(Dense(2))\n",
    "\n",
    "# Apply the custom activation function mentioned above\n",
    "model.add(Activation(activate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the discrete log-likelihood for Weibull survival data as our loss function\n",
    "model.compile(loss=weibull_loglik_discrete, optimizer=RMSprop(lr=.001))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit!\n",
    "model.fit(train_x, train_y, epochs=250, batch_size=2000, verbose=2, validation_data=(test_x, test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " Make some predictions and put them alongside the real TTE and event indicator values\n",
    "test_predict = model.predict(test_x)\n",
    "test_predict = np.resize(test_predict, (100, 2))\n",
    "test_result = np.concatenate((test_y, test_predict), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_frame = pd.DataFrame(test_result, columns=['TTE', 'Event Indicator', 'Alpha', 'Beta'])\n",
    "result_frame.to_pickle('results_pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.read_pickle('results_pickle')\n",
    "print(frame.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTE, Event Indicator, Alpha, Beta\n",
    "print(test_result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
